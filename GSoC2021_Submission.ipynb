{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GSoC2021_Submission.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Q9GePNIAwWI"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnW9Pru4ycDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "844fb6a7-678a-4845-e72a-702bf551618e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbs3lFgPRgkV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66dd3c16-1aa6-4d91-9159-ca8fa2514f4f"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n",
            "10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYzDRpNpNI9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429da518-1a21-4ace-f9de-2f749193a27b"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "\n",
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp37-none-any.whl size=7411 sha256=4b47b584df1445261744e8bbdb4d45ea96c2e801c8c9d0f6e5103e7882cd6156\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.7 GB  | Proc size: 277.9 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BsedwEEfyv6"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import os, glob\n",
        "import time\n",
        "import h5py\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils\n",
        "import torch.utils.data\n",
        "from torch.utils.data import ConcatDataset, Dataset, DataLoader, sampler, DistributedSampler\n",
        "#from torch.utils.data import *\n",
        "from sklearn.metrics import roc_curve, auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSmJ6ShNBIUm"
      },
      "source": [
        "## Optional for parsing argument "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp_m-43Jf8lh"
      },
      "source": [
        "import argparse\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "parser = argparse.ArgumentParser()\n",
        "#parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
        "parser.add_argument('--epochs', type=int, default=50, help='Number of epochs to train.')\n",
        "parser.add_argument('--batch_size', type=int, default=64, help='Initial learning rate.') #100\n",
        "parser.add_argument('--maxnodes', type=int, default=1000, help='max nodes.') #100\n",
        "parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.') #0.001\n",
        "parser.add_argument('--dropout', type=float, default=0.3, help='Dropout rate (1 - keep probability).')\n",
        "args = parser.parse_args([])\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGorpMPqgAGj"
      },
      "source": [
        "BATCH_SZ = 64\n",
        "\n",
        "granularity = 1\n",
        "#channels = [0,1,2,3,4,5,6,7]\n",
        "#channels = [0,3,5]\n",
        "lr_init = 5.e-4\n",
        "edgeconvblocks = 3\n",
        "epochs = 1\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(0)\n",
        "\n",
        "expt_name = 'Graphs_%d_RH1o100_ECAL+HCAL+Trk_lr%s_gamma0.5every10ep_epochs%d'%(edgeconvblocks, str(lr_init), epochs)#'ResNet_blocks%d_RH1o100_ECAL+HCAL+Trk_lr%s_gamma0.5every10ep_epochs%d'%(resblocks, str(lr_init), epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIwXn7p9gB1W"
      },
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "class ParquetDataset(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.parquet = pq.ParquetFile(filename)\n",
        "        self.cols = None # read all columns\n",
        "        #self.cols = ['X_jets.list.item.list.item.list.item','y'] \n",
        "    def __getitem__(self, index):\n",
        "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
        "        data['X_jets'] = torch.tensor(np.float32(data['X_jets'])) \n",
        "        data['y'] = torch.tensor(np.float32(data['y']))\n",
        "        data['m0'] = torch.tensor(np.float32(data['m0']))\n",
        "        data['pt'] = torch.tensor(np.float32(data['pt']))\n",
        "        data['X_jets'][data['X_jets'] < 1.e-3] = 0.\n",
        "        # Preprocessing\n",
        "        #data['nonzeroPixels'][data['nonzeroPixels'] < 1.e-3] = 0. # Zero-Suppression\n",
        "        #data['nonzeroPixels'][-1,...] = 25.*data['nonzeroPixels'][-1,...] # For HCAL: to match pixel intensity distn of other layers\n",
        "        #data['nonzeroPixels'] = data['nonzeroPixels']/100. # To standardize\n",
        "        return dict(data)\n",
        "    def __len__(self):\n",
        "        return self.parquet.num_row_groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhM7DVqFs2X1"
      },
      "source": [
        "import torch_geometric\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear as Lin, Sequential as Seq, ReLU, AvgPool1d, Dropout, Softmax, Sigmoid, BatchNorm1d\n",
        "from torch_geometric.nn import EdgeConv, DynamicEdgeConv, BatchNorm, avg_pool_x, knn_graph, EdgePooling, DenseSAGEConv, dense_diff_pool, dense_mincut_pool, SAGEConv, max_pool_x, SAGPooling, JumpingKnowledge, global_mean_pool as gap, global_max_pool as gmp\n",
        "#from torch_cluster import knn_graph\n",
        "\n",
        "class PointNet(torch.nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(PointNet, self).__init__()\n",
        "        self.edgeConv0 = EdgeConv(ShortcutModule(in_channels, 32), aggr='mean')\n",
        "        \n",
        "        self.edgeConvBlock1 = SAGEConv(in_channels, 512, normalize=True)\n",
        "        self.edgePool1 = SAGPooling(512, GNN=torch_geometric.nn.conv.SAGEConv, nonlinearity=ReLU())\n",
        "  \n",
        "        self.edgeConvBlock2 = SAGEConv(512, 512, normalize=True)\n",
        "        self.edgePool2 = SAGPooling(512, GNN=torch_geometric.nn.conv.SAGEConv, nonlinearity=ReLU())\n",
        "        \n",
        "        self.edgeConvBlock3 = SAGEConv(512, 512, normalize=True)\n",
        "        self.edgePool3 = SAGPooling(512, GNN=torch_geometric.nn.conv.SAGEConv, nonlinearity=ReLU())\n",
        "        self.JKNet = JumpingKnowledge(mode='max')\n",
        "        self.edgeConvBlock4 = SAGEConv(512, 512, normalize=True)\n",
        "        self.dropout = Dropout(0.1)\n",
        "        self.poolit1=Lin(128,250)\n",
        "        self.poolit2=Lin(256,50)\n",
        "        \n",
        "        self.dense1 = Seq(Lin(1024,1))\n",
        "        self.dense2 = Seq(Lin(1024,1),ReLU())\n",
        "        self.softmax = Softmax()\n",
        "        self.bano1 = BatchNorm(512)\n",
        "        self.bano2 = BatchNorm(512)\n",
        "        self.bano3 = BatchNorm(512) \n",
        "        self.bano4 = BatchNorm(512)  \n",
        "        self.sigmoid = Sigmoid()\n",
        "        self.relu = ReLU()\n",
        "\n",
        "    def forward(self, gra, lengs, adj, pos, nodeCount):\n",
        "        \n",
        "        x1 = self.bano1(self.relu(self.edgeConvBlock1(gra, edge_index=adj))) \n",
        "        x1_readout = torch.cat([gap(x1, batch=lengs),gmp(x1, batch=lengs)], dim=1)\n",
        "        \n",
        "        x2 = self.bano2(self.relu(self.edgeConvBlock2(x1, edge_index=adj)))\n",
        "        x2_readout = torch.cat([gap(x2, batch=lengs),gmp(x2, batch=lengs)], dim=1)\n",
        "\n",
        "        x3 = self.bano3(self.relu(self.edgeConvBlock3(x2, edge_index=adj)))\n",
        "        x3_readout = torch.cat([gap(x3, batch=lengs),gmp(x3, batch=lengs)], dim=1)\n",
        "\n",
        "        x = x1_readout + x2_readout + x3_readout\n",
        "        \n",
        "        x = self.dense1(x)\n",
        "       \n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x, x3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4G-fxl2g96m"
      },
      "source": [
        "num_node_features = 10\n",
        "model1 = PointNet(num_node_features)\n",
        "model1.cuda()\n",
        "\n",
        "optimizer = optim.Adam(model1.parameters(), lr=lr_init)\n",
        "lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20], gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfKBGJr4ODa3"
      },
      "source": [
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch.nn.functional import pad\n",
        "MAX_NODES = 5000\n",
        "class TrainParquetDataset(Dataset):\n",
        "    def __init__(self, filename):\n",
        "        self.parquet = pq.ParquetFile(filename)\n",
        "        self.cols = None # read all columns\n",
        "        #self.cols = ['X_jets.list.item.list.item.list.item','y'] \n",
        "    def __getitem__(self, index):\n",
        "        data = self.parquet.read_row_group(index, columns=self.cols).to_pydict()\n",
        "        data_out = {}\n",
        "        num_nodes = torch.tensor(data['coords0']).shape[1]\n",
        "        data_out['num_nodes'] = torch.tensor(num_nodes)\n",
        "        data_out['ECAL'] = pad(torch.tensor(data['ECAL'][0])/100.,(0,MAX_NODES-num_nodes),'constant')\n",
        "        data_out['y'] = torch.tensor(data['y'])\n",
        "        data_out['pT'] = pad(torch.tensor(data['pT'][0])/10.,(0,MAX_NODES-num_nodes),'constant')\n",
        "        data_out['pT_jet'] = torch.tensor(data['pT_jet'])\n",
        "        data_out['HCAL'] = pad(torch.tensor(data['HCAL'][0])/500.,(0,MAX_NODES-num_nodes),'constant')\n",
        "        data_out['m0'] = torch.tensor(data['m0'][0])\n",
        "        data_out['dz'] = pad(torch.tensor(data['dz'][0])/2.5,(0,MAX_NODES-num_nodes),'constant')\n",
        "        data_out['d0'] = pad(torch.tensor(data['d0'][0])/2.,(0,MAX_NODES-num_nodes),'constant')\n",
        "        data_out['BPIX1'] = pad(torch.tensor(data['BPIX1'][0])/8.,(0,MAX_NODES-num_nodes),'constant')\n",
        "        data_out['BPIX2'] = pad(torch.tensor(data['BPIX2'][0])/10.,(0,MAX_NODES-num_nodes),'constant')\n",
        "        data_out['BPIX3'] = pad(torch.tensor(data['BPIX3'][0])/13.,(0,MAX_NODES-num_nodes),'constant')\n",
        "        data_out['coords0'] = pad(torch.tensor(data['coords0'][0]),(0,MAX_NODES-num_nodes),'constant')\n",
        "        data_out['coords1'] = pad(torch.tensor(data['coords1'][0]),(0,MAX_NODES-num_nodes),'constant')\n",
        "        data_out['edge_index_from'] = pad(torch.tensor(data['edge_index_from'][0]),(0,16*(MAX_NODES-num_nodes)),'constant')\n",
        "        data_out['edge_index_to'] = pad(torch.tensor(data['edge_index_to'][0]),(0,16*(MAX_NODES-num_nodes)),'constant')\n",
        "      \n",
        "        return dict(data_out)\n",
        "    def __len__(self):\n",
        "        return self.parquet.num_row_groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlNdftryOEwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504b524f-ab72-4493-8db0-15e7f4894059"
      },
      "source": [
        "trainDecays = glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_0_to_6399.parquet')\n",
        "trainDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_6400_to_12799.parquet'))\n",
        "trainDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_12800_to_19199.parquet'))\n",
        "trainDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_19200_to_25599.parquet'))\n",
        "trainDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_25600_to_31999.parquet'))\n",
        "trainDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_32000_to_38399.parquet'))\n",
        "trainDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_38400_to_44799.parquet'))\n",
        "trainDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_44800_to_51199.parquet'))\n",
        "trainDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_51200_to_57599.parquet'))\n",
        "trainDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_57600_to_63999.parquet'))\n",
        "#trainDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_96000_to_102399.parquet'))\n",
        "assert len(trainDecays) == 10, \"len(decays) = %d\"%(len(trainDecays))\n",
        "\n",
        "print(\">> Input files:\",trainDecays)\n",
        "dset_train = ConcatDataset([TrainParquetDataset(d) for d in trainDecays])\n",
        "idxs = np.random.permutation(6400*5)\n",
        "train_sampler = sampler.SubsetRandomSampler(idxs[:(6400*5)])\n",
        "train_loader = DataLoader(dataset=dset_train, batch_size=BATCH_SZ, num_workers=0, sampler=train_sampler, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Input files: ['./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_0_to_6399.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_6400_to_12799.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_12800_to_19199.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_19200_to_25599.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_25600_to_31999.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_32000_to_38399.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_38400_to_44799.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_44800_to_51199.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_51200_to_57599.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_57600_to_63999.parquet']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxBLrlAZNlW8",
        "outputId": "a3963d40-6184-4fad-d96b-ef420dcadf41"
      },
      "source": [
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 10.1 GB  | Proc size: 6.4 GB\n",
            "GPU RAM Free: 15109MB | Used: 0MB | Util   0% | Total 15109MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdc2fNlxhJOD"
      },
      "source": [
        "## generate list to count nodes for each graph\n",
        "def nodeCounter(samples):\n",
        "    inds=[]\n",
        "    for k in samples:\n",
        "        inds.append(k['x'].shape[0])\n",
        "    return inds\n",
        "\n",
        "def ref(bsize,nodeC,i1,i2):\n",
        "  maxC=np.max(np.array(nodeC))\n",
        "  maxC=args.maxnodes#maxC + (4 - maxC % 4) ##max num of nodes 1161%4\n",
        "  refMat=np.zeros((bsize,maxC)) ## matrix of zeros\n",
        "  for pi in range(i1,i2):##10\n",
        "    refMat[pi,:nodeC[pi]]=1 ## fill ones \n",
        "  return refMat,maxC\n",
        "\n",
        "def assigner(nodelist):\n",
        "  fin=[]\n",
        "  countit=0\n",
        "  for m in nodelist:\n",
        "      fin.append(np.repeat(countit,m))\n",
        "      countit+=1\n",
        "  return np.array(fin)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3H83osSQ6Md"
      },
      "source": [
        "## Graph generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-MulFssOneg"
      },
      "source": [
        "import torch_geometric.transforms\n",
        "from torch_geometric.nn import knn_graph\n",
        "import torch_geometric.data\n",
        "import torch \n",
        "from torch_geometric.data import Data\n",
        "import numpy as np\n",
        "\n",
        "def graphCloud(batch_data, start_idx, end_idx, granularity=1):\n",
        "  graphs = []\n",
        "  num_nodes = batch_data['num_nodes']\n",
        "  for i in range(start_idx, end_idx):\n",
        "    data={}\n",
        "    data['BPIX1'] = torch.unsqueeze(batch_data['BPIX1'][i][:num_nodes[i]].cuda(),dim=1)\n",
        "    data['BPIX2'] = torch.unsqueeze(batch_data['BPIX2'][i][:num_nodes[i]].cuda(),dim=1)\n",
        "    data['BPIX3'] = torch.unsqueeze(batch_data['BPIX3'][i][:num_nodes[i]].cuda(),dim=1)\n",
        "    data['ECAL'] = torch.unsqueeze(batch_data['ECAL'][i][:num_nodes[i]].cuda(),dim=1)\n",
        "    data['pT'] = torch.unsqueeze(batch_data['pT'][i][:num_nodes[i]].cuda(),dim=1)\n",
        "    data['dz'] = torch.unsqueeze(batch_data['dz'][i][:num_nodes[i]].cuda(),dim=1)\n",
        "    data['d0'] = torch.unsqueeze(batch_data['d0'][i][:num_nodes[i]].cuda(),dim=1)\n",
        "    data['HCAL'] = torch.unsqueeze(batch_data['HCAL'][i][:num_nodes[i]].cuda(),dim=1)\n",
        "    data['coords0'] = torch.unsqueeze(batch_data['coords0'][i][:num_nodes[i]].cuda(),dim=1)\n",
        "    data['coords1'] = torch.unsqueeze(batch_data['coords1'][i][:num_nodes[i]].cuda(),dim=1)\n",
        "    data['edge_index_from'] = torch.unsqueeze(batch_data['edge_index_from'][i][:(num_nodes[i]*16)].cuda(),dim=0)\n",
        "    data['edge_index_to'] = torch.unsqueeze(batch_data['edge_index_to'][i][:(num_nodes[i]*16)].cuda(),dim=0)\n",
        "    feats = torch.cat((data['coords0'],data['coords1'],data['pT'],data['ECAL'],data['HCAL'],data['d0'],data['dz'],data['BPIX1'],data['BPIX2'],data['BPIX3']), dim=1)\n",
        "    feats = torch.unique(feats,dim=0)\n",
        "    edge_index = torch.cat((data['edge_index_from'],data['edge_index_to']),dim=0)\n",
        "    graph = Data(x=feats, pos=torch.cat((data['coords0'],data['coords1']),dim=1), edge_index=edge_index)\n",
        "    graphs.append(graph)\n",
        "  \n",
        "  return graphs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5OOYyHj4nRi"
      },
      "source": [
        "# Helper functions for visualization.\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "def visualize_mesh(pos, face):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.gca(projection='3d')\n",
        "    ax.axes.xaxis.set_ticklabels([])\n",
        "    ax.axes.yaxis.set_ticklabels([])\n",
        "    ax.axes.zaxis.set_ticklabels([])\n",
        "    ax.plot_trisurf(pos[:, 0], pos[:, 1], pos[:, 2], triangles=data.face.t(), antialiased=False)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_points(pos, edge_index=None, index=None):\n",
        "    fig = plt.figure(figsize=(14, 14))\n",
        "    if edge_index is not None:\n",
        "        for (src, dst) in edge_index.t().tolist():\n",
        "             src = pos[src].tolist()\n",
        "             dst = pos[dst].tolist()\n",
        "             plt.plot([src[0], dst[0]], [src[1], dst[1]], linewidth=1, color='black') \n",
        "    if index is None:\n",
        "        plt.scatter(pos[:, 0], pos[:, 1], s=30, color='green', zorder=1000)\n",
        "    else:\n",
        "       mask = torch.zeros(pos.size(0), dtype=torch.bool)\n",
        "       mask[index] = True\n",
        "       plt.scatter(pos[~mask, 0], pos[~mask, 1], s=50, color='lightgray', zorder=1000) \n",
        "       plt.scatter(pos[mask, 0],  pos[mask, 1], s=50, zorder=1000)\n",
        "    plt.axis('on')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CekpRN8pem9R"
      },
      "source": [
        "import gc \n",
        "gc.collect() \n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFBQJ936N1An",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca403079-0bc1-470d-8687-265a910ce38b"
      },
      "source": [
        "printm()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 9.7 GB  | Proc size: 5.4 GB\n",
            "GPU RAM Free: 15109MB | Used: 0MB | Util   0% | Total 15109MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZ3FSfgMRybA"
      },
      "source": [
        "## Train the Graph Neural Network Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6ISxrr41Bpx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b29f688-eaf2-4dc6-e9cd-b4148b3493ee"
      },
      "source": [
        "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
        "# MAIN #\n",
        "torch.backends.cudnn.enabled=True\n",
        "#eval_step = 1000\n",
        "print_step = 10\n",
        "roc_auc_best = 0.5\n",
        "\n",
        "count=0\n",
        "c1,c2=0, BATCH_SZ\n",
        "t = time.time()\n",
        "max_nodes = args.maxnodes\n",
        "start_epoch = 0\n",
        "running_loss_list = []\n",
        "acc_list = []\n",
        "sum_list=[]\n",
        "max_samples=3200/BATCH_SZ/print_step\n",
        "BATCH_SZ = 64\n",
        "USE_PREVIOUS = True\n",
        "\n",
        "if (USE_PREVIOUS):\n",
        "  model1 = PointNet(num_node_features)\n",
        "  model1.cuda()\n",
        "  #model1 = torch.nn.DataParallel(model1, device_ids=[0])\n",
        "  optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
        "  lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20], gamma=0.5)\n",
        "  checkpoint = torch.load('MODELS/SAGEConv_trial1_0-1-2-3-4_channels_256-512-1024-layers_without_SAGPooling+readout.pt')\n",
        "  #checkpoint = torch.load('MODELS/PointNet_0-3-5_channels_1*3200_normalised_clip5_samples.pt')\n",
        "  model1.load_state_dict(checkpoint['model_state_dict'])\n",
        "  #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  start_epoch = checkpoint['epoch']\n",
        "  loss = checkpoint['loss']\n",
        "  running_loss_list = checkpoint['running_loss']\n",
        "  acc_list = checkpoint['accuracy']\n",
        "  epochs = checkpoint['total_epochs']\n",
        "\n",
        "  print('Prevsious durring Trainng:')\n",
        "  print(' >> Epochs finished earlier : ',start_epoch,' out of ',epochs,'. Continuing remaining training.')\n",
        "  print(' >> Batch Loss: ', loss)\n",
        "  print(' >> Running Loss: ',running_loss_list)\n",
        "  print(' >> Accuracy: ',acc_list )\n",
        "optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
        "print(' >> Optimizer: ',optimizer)\n",
        "print(' >> lr_scheduler: ',lr_scheduler)\n",
        "\n",
        "print(\">> Training <<<<<<<<\")\n",
        "#f = open('%s.log'%(expt_name), 'w')\n",
        "epochs = 3\n",
        "#epochs=23\n",
        "start_epoch = 0\n",
        "MAX_NODES=5000\n",
        "\n",
        "for e in range(start_epoch, epochs):\n",
        "\n",
        "    epoch = e+1\n",
        "    s = '>> Epoch %d <<<<<<<<'%(epoch)\n",
        "    print(s)\n",
        "    #f.write('%s\\n'%(s))\n",
        "    running_loss, acc_ = 0., 0.\n",
        "    # Run training\n",
        "    \n",
        "    model1.train()\n",
        "    now = time.time()\n",
        "    batch_data = []\n",
        "    true_y=[]\n",
        "    train_step=0\n",
        "    gc.collect() \n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        #print(i, logits)\n",
        "        #if i>=max_samples:\n",
        "        #  break\n",
        "        #batch_data.append(data)\n",
        "        #true_y.append(int(data['y'].item()))\n",
        "        #if (i+1)%BATCH_SZ!=0 and (i+1)<max_samples:\n",
        "        # continue\n",
        "        train_step+=1\n",
        "        #print(\"Step: \",train_step)\n",
        "        if len(data['y'])<c2 or len(data['y'])<BATCH_SZ:\n",
        "          print(\" >> Last Batch with less samples than Batch size: Found \", len(data['y']), \"samples but Batch size is \",c2)\n",
        "          c2 = len(data['y'])\n",
        "        else:\n",
        "          c2 = BATCH_SZ\n",
        "        \n",
        "        \n",
        "        rawGraph = graphCloud(data,c1,c2) ##Generating graphs from raw data\n",
        "        nodeCount = nodeCounter(rawGraph)\n",
        "        lengs=torch.LongTensor(np.hstack(assigner(np.array(nodeCount[c1:c2])))).cuda()\n",
        "\n",
        "        compress = torch_geometric.data.Batch.from_data_list(rawGraph)\n",
        "\n",
        "        gra = compress.x.clone()\n",
        "        pos = compress.pos.clone()\n",
        "        adj = compress.edge_index.clone()\n",
        "        gra.cuda()\n",
        "        pos.cuda()\n",
        "        lengs.cuda()\n",
        "        adj.cuda()\n",
        "        '''\n",
        "        whole,mask=to_dense_batch(gra, lengs, fill_value=0, max_num_nodes=MAX_NODES)#refMat.shape[1])\n",
        "        wholeAdj=to_dense_adj(adj, lengs, edge_attr=None, max_num_nodes=MAX_NODES)#refMat.shape[1]).cuda()\n",
        "\n",
        "        whole=whole.cuda()\n",
        "        mask=mask.cuda()\n",
        "        lengs=lengs.cuda()\n",
        "        wholeAdj.cuda()\n",
        "        '''\n",
        "        \n",
        "        model1.zero_grad()\n",
        "        #print('conv1.bias.grad before backward')\n",
        "        #print(model1.edgeConv0.nn.nn[0].bias)\n",
        "        optimizer.zero_grad()\n",
        "        #print(model1(gra,lengs).shape)\n",
        "        #logits = model1(gra,lengs,pos)\n",
        "        logits, stage1_graph = model1(gra, lengs, adj, pos, nodeCount)\n",
        "        #print(logits.shape)\n",
        "        true_y = torch.squeeze(data['y'])#torch.squeeze(data['y'].long())\n",
        "        #loss = torch.nn.BCELoss()(torch.squeeze(logits), true_y.to('cuda'))#F.cross_entropy(logits, true_y.to('cuda'))\n",
        "        loss = F.binary_cross_entropy(torch.squeeze(logits),true_y.to('cuda'))\n",
        "        loss.backward()\n",
        "        #print('conv1.bias.grad after backward')\n",
        "        #print(model1.edgeConv0.nn.nn[0].bias)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        running_loss += loss.item()*c2\n",
        "        #pred = torch.argmax(logits, dim=1)\n",
        "        pred = torch.zeros_like(logits)\n",
        "        pred[logits>0.5] = 1\n",
        "        acc = sum(torch.squeeze(pred)==torch.tensor(true_y).to('cuda')).item()\n",
        "        sum_list.append(acc)\n",
        "        acc_ = (acc_*BATCH_SZ*(train_step-1) + acc)/(c2+BATCH_SZ*(train_step-1))\n",
        "        #print('epoch: ',epoch, 'sample: ',i, 'loss: ',loss.item(),'running loss', running_loss, 'correct predictions in batch: ',acc, 'accuracy: ',acc_)\n",
        "        #print('sample no: ',i,'epoch no: ',e)\n",
        "        if train_step % print_step == 0:\n",
        "            #torch.save(model1.state_dict(),'MODELS/PointNet_model1_lr_1e-3_epochs_'+str(epoch)+'_Batch_'+str(BATCH_SZ)+'_100*3200_samples.pt')\n",
        "            s = '%d %d: Train running loss:%f, Train batch loss:%f, acc:%f '%(epoch, i, running_loss, loss.item(), acc_)\n",
        "            print(\"Step: \", train_step, s)\n",
        "        train_data, train_y = batch_data, true_y\n",
        "        #batch_data=[]  # Flush batch data\n",
        "        #true_y=[]\n",
        "        gc.collect() \n",
        "        torch.cuda.empty_cache()\n",
        "        # For more frequent validation:\n",
        "        #if epoch > 1 and i % eval_step == 0:\n",
        "        #    model1.eval()\n",
        "        #    roc_auc_best = do_eval(model1, val_loader, f, roc_auc_best, epoch)\n",
        "        #    model1.train()\n",
        "    acc_list.append(acc_)\n",
        "    running_loss_list.append(running_loss)\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model1.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            'running_loss': running_loss_list,\n",
        "            'accuracy': acc_list,\n",
        "            'total_epochs':epochs\n",
        "            }, 'MODELS/SAGEConv_trial1_0-1-2-3-4_channels_256-512-1024-layers_without_SAGPooling+readout.pt')\n",
        "    \n",
        "    s = '%d: Train running loss:%f, Train batch loss:%f, acc:%f '%(epoch, running_loss, loss.item(), acc_)\n",
        "    #f.write('%s\\n'%(s))\n",
        "    now = time.time() - now\n",
        "    s = '%d: Train time:%.2fs in %d steps'%(epoch, now, len(train_loader))\n",
        "    print(s)\n",
        "    #f.write('%s\\n'%(s))\n",
        "\n",
        "    # Run Validation\n",
        "    #model1.eval()\n",
        "    #roc_auc_best = do_eval(model1, val_loader, f, roc_auc_best, epoch)\n",
        "\n",
        "#f.close()\n",
        "#torch.save(model1.state_dict(), 'MODELS/PointNet_model1_lr_1e-3_epochs'+str(epochs)+'_Batch_'+str(BATCH_SZ)+'_100*3200_samples.pt')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prevsious durring Trainng:\n",
            " >> Epochs finished earlier :  3  out of  3 . Continuing remaining training.\n",
            " >> Batch Loss:  tensor(0.1046, device='cuda:0', requires_grad=True)\n",
            " >> Running Loss:  [13298.006275177002, 11620.754623413086, 11082.379445075989, 10609.699986457825, 9949.00496673584, 10021.686545848846, 9608.798424720764, 8946.212854385376, 8711.667983055115, 8098.950366973877, 8372.715818405151, 8123.056043148041, 7942.492605686188, 7657.1106133461, 7662.134208202362, 7446.212458610535, 7239.637764215469, 7205.474262475967, 7045.145822763443, 7007.376295089722, 6833.058912754059, 6762.9165551662445, 6695.367338418961, 6490.576879739761, 6353.290660381317, 6276.783454656601, 6253.090657711029, 5768.962197065353, 6076.603927373886, 5854.505924701691, 5308.150457620621, 5609.639824390411, 5435.311896204948, 4969.168320417404, 4864.724485635757]\n",
            " >> Accuracy:  [0.81965625, 0.8483437499999997, 0.85609375, 0.8620937499999999, 0.8709687500000001, 0.8713750000000002, 0.8767812499999998, 0.885125, 0.8910312500000003, 0.898375, 0.89525, 0.89834375, 0.90109375, 0.903625, 0.9033125, 0.90625, 0.91084375, 0.9090625, 0.91171875, 0.9137500000000001, 0.9144999999999999, 0.9165625, 0.9158125, 0.919, 0.92159375, 0.920625, 0.92225, 0.927875, 0.9241875, 0.92871875, 0.9345625, 0.92996875, 0.93275, 0.9385625, 0.9383125]\n",
            " >> Optimizer:  Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            " >> lr_scheduler:  <torch.optim.lr_scheduler.MultiStepLR object at 0x7fc1c18706d0>\n",
            ">> Training <<<<<<<<\n",
            ">> Epoch 1 <<<<<<<<\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step:  10 1 9: Train running loss:150.552868, Train batch loss:0.252719, acc:0.914062 \n",
            "Step:  20 1 19: Train running loss:257.210987, Train batch loss:0.156290, acc:0.926562 \n",
            "Step:  30 1 29: Train running loss:343.510039, Train batch loss:0.136812, acc:0.933854 \n",
            "Step:  40 1 39: Train running loss:404.715028, Train batch loss:0.132143, acc:0.939844 \n",
            "Step:  50 1 49: Train running loss:485.119409, Train batch loss:0.084145, acc:0.941875 \n",
            "Step:  60 1 59: Train running loss:591.734335, Train batch loss:0.140786, acc:0.939583 \n",
            "Step:  70 1 69: Train running loss:685.893205, Train batch loss:0.166171, acc:0.938616 \n",
            "Step:  80 1 79: Train running loss:781.058520, Train batch loss:0.215744, acc:0.937695 \n",
            "Step:  90 1 89: Train running loss:880.309647, Train batch loss:0.099140, acc:0.938194 \n",
            "Step:  100 1 99: Train running loss:953.976710, Train batch loss:0.167642, acc:0.940469 \n",
            "Step:  110 1 109: Train running loss:1055.903844, Train batch loss:0.071878, acc:0.939915 \n",
            "Step:  120 1 119: Train running loss:1147.197423, Train batch loss:0.136827, acc:0.939714 \n",
            "Step:  130 1 129: Train running loss:1240.305223, Train batch loss:0.173818, acc:0.939904 \n",
            "Step:  140 1 139: Train running loss:1363.625541, Train batch loss:0.173187, acc:0.938504 \n",
            "Step:  150 1 149: Train running loss:1499.271926, Train batch loss:0.081242, acc:0.936979 \n",
            "Step:  160 1 159: Train running loss:1630.625638, Train batch loss:0.218951, acc:0.935840 \n",
            "Step:  170 1 169: Train running loss:1717.421652, Train batch loss:0.219219, acc:0.935937 \n",
            "Step:  180 1 179: Train running loss:1809.423933, Train batch loss:0.144671, acc:0.936285 \n",
            "Step:  190 1 189: Train running loss:1894.363971, Train batch loss:0.175151, acc:0.937336 \n",
            "Step:  200 1 199: Train running loss:2004.074636, Train batch loss:0.120130, acc:0.937500 \n",
            "Step:  210 1 209: Train running loss:2119.994021, Train batch loss:0.173253, acc:0.936979 \n",
            "Step:  220 1 219: Train running loss:2195.964515, Train batch loss:0.081912, acc:0.937571 \n",
            "Step:  230 1 229: Train running loss:2281.309247, Train batch loss:0.061288, acc:0.937976 \n",
            "Step:  240 1 239: Train running loss:2360.133157, Train batch loss:0.131927, acc:0.938802 \n",
            "Step:  250 1 249: Train running loss:2434.649008, Train batch loss:0.061438, acc:0.939625 \n",
            "Step:  260 1 259: Train running loss:2528.387902, Train batch loss:0.087093, acc:0.939603 \n",
            "Step:  270 1 269: Train running loss:2600.677063, Train batch loss:0.140394, acc:0.940046 \n",
            "Step:  280 1 279: Train running loss:2676.226286, Train batch loss:0.104352, acc:0.940513 \n",
            "Step:  290 1 289: Train running loss:2763.298301, Train batch loss:0.147766, acc:0.940517 \n",
            "Step:  300 1 299: Train running loss:2864.626567, Train batch loss:0.148210, acc:0.940521 \n",
            "Step:  310 1 309: Train running loss:2946.620918, Train batch loss:0.089703, acc:0.940877 \n",
            "Step:  320 1 319: Train running loss:3023.562525, Train batch loss:0.119098, acc:0.941602 \n",
            "Step:  330 1 329: Train running loss:3091.855536, Train batch loss:0.082409, acc:0.942045 \n",
            "Step:  340 1 339: Train running loss:3195.213258, Train batch loss:0.199414, acc:0.941820 \n",
            "Step:  350 1 349: Train running loss:3295.587094, Train batch loss:0.271095, acc:0.941920 \n",
            "Step:  360 1 359: Train running loss:3397.247434, Train batch loss:0.213049, acc:0.941667 \n",
            "Step:  370 1 369: Train running loss:3480.700382, Train batch loss:0.167833, acc:0.941723 \n",
            "Step:  380 1 379: Train running loss:3582.564241, Train batch loss:0.231934, acc:0.941735 \n",
            "Step:  390 1 389: Train running loss:3673.907610, Train batch loss:0.135585, acc:0.941707 \n",
            "Step:  400 1 399: Train running loss:3784.988681, Train batch loss:0.231810, acc:0.941445 \n",
            "Step:  410 1 409: Train running loss:3870.441056, Train batch loss:0.101829, acc:0.941502 \n",
            "Step:  420 1 419: Train running loss:3969.823607, Train batch loss:0.281184, acc:0.941406 \n",
            "Step:  430 1 429: Train running loss:4076.051871, Train batch loss:0.313322, acc:0.941315 \n",
            "Step:  440 1 439: Train running loss:4186.758973, Train batch loss:0.114727, acc:0.941158 \n",
            "Step:  450 1 449: Train running loss:4290.626745, Train batch loss:0.323550, acc:0.940903 \n",
            "Step:  460 1 459: Train running loss:4441.826188, Train batch loss:0.198800, acc:0.940183 \n",
            "Step:  470 1 469: Train running loss:4573.076436, Train batch loss:0.054661, acc:0.939827 \n",
            "Step:  480 1 479: Train running loss:4697.799620, Train batch loss:0.116619, acc:0.939355 \n",
            "Step:  490 1 489: Train running loss:4836.457281, Train batch loss:0.111158, acc:0.938999 \n",
            "Step:  500 1 499: Train running loss:4955.576826, Train batch loss:0.383562, acc:0.938844 \n",
            "1: Train time:1345.70s in 500 steps\n",
            ">> Epoch 2 <<<<<<<<\n",
            "Step:  10 2 9: Train running loss:91.306878, Train batch loss:0.236064, acc:0.950000 \n",
            "Step:  20 2 19: Train running loss:197.282384, Train batch loss:0.256797, acc:0.942187 \n",
            "Step:  30 2 29: Train running loss:284.057801, Train batch loss:0.139666, acc:0.942187 \n",
            "Step:  40 2 39: Train running loss:351.975338, Train batch loss:0.124290, acc:0.946484 \n",
            "Step:  50 2 49: Train running loss:427.168118, Train batch loss:0.092658, acc:0.949063 \n",
            "Step:  60 2 59: Train running loss:516.952262, Train batch loss:0.190296, acc:0.947656 \n",
            "Step:  70 2 69: Train running loss:585.668468, Train batch loss:0.142511, acc:0.949554 \n",
            "Step:  80 2 79: Train running loss:675.685334, Train batch loss:0.182590, acc:0.949023 \n",
            "Step:  90 2 89: Train running loss:747.955278, Train batch loss:0.061108, acc:0.950000 \n",
            "Step:  100 2 99: Train running loss:836.628944, Train batch loss:0.129127, acc:0.949375 \n",
            "Step:  110 2 109: Train running loss:911.988263, Train batch loss:0.157291, acc:0.950426 \n",
            "Step:  120 2 119: Train running loss:985.484521, Train batch loss:0.039626, acc:0.950781 \n",
            "Step:  130 2 129: Train running loss:1083.320446, Train batch loss:0.150764, acc:0.950120 \n",
            "Step:  140 2 139: Train running loss:1187.125961, Train batch loss:0.211069, acc:0.948884 \n",
            "Step:  150 2 149: Train running loss:1292.773686, Train batch loss:0.181192, acc:0.947812 \n",
            "Step:  160 2 159: Train running loss:1383.675612, Train batch loss:0.247946, acc:0.947266 \n",
            "Step:  170 2 169: Train running loss:1457.646547, Train batch loss:0.108256, acc:0.947794 \n",
            "Step:  180 2 179: Train running loss:1547.447439, Train batch loss:0.147662, acc:0.947222 \n",
            "Step:  190 2 189: Train running loss:1627.167924, Train batch loss:0.142376, acc:0.947204 \n",
            "Step:  200 2 199: Train running loss:1711.765266, Train batch loss:0.081388, acc:0.947422 \n",
            "Step:  210 2 209: Train running loss:1803.717381, Train batch loss:0.190690, acc:0.947321 \n",
            "Step:  220 2 219: Train running loss:1883.739841, Train batch loss:0.104932, acc:0.947372 \n",
            "Step:  230 2 229: Train running loss:1966.809407, Train batch loss:0.147806, acc:0.947011 \n",
            "Step:  240 2 239: Train running loss:2067.654100, Train batch loss:0.157448, acc:0.946745 \n",
            "Step:  250 2 249: Train running loss:2157.202295, Train batch loss:0.146254, acc:0.947063 \n",
            "Step:  260 2 259: Train running loss:2247.438204, Train batch loss:0.205294, acc:0.946815 \n",
            "Step:  270 2 269: Train running loss:2323.860987, Train batch loss:0.133444, acc:0.947280 \n",
            "Step:  280 2 279: Train running loss:2437.548091, Train batch loss:0.192705, acc:0.946652 \n",
            "Step:  290 2 289: Train running loss:2527.966705, Train batch loss:0.166518, acc:0.946390 \n",
            "Step:  300 2 299: Train running loss:2612.144266, Train batch loss:0.197208, acc:0.946562 \n",
            "Step:  310 2 309: Train running loss:2695.006757, Train batch loss:0.110645, acc:0.946623 \n",
            "Step:  320 2 319: Train running loss:2780.370926, Train batch loss:0.140478, acc:0.946338 \n",
            "Step:  330 2 329: Train running loss:2874.443205, Train batch loss:0.200499, acc:0.946259 \n",
            "Step:  340 2 339: Train running loss:2960.205476, Train batch loss:0.081936, acc:0.946324 \n",
            "Step:  350 2 349: Train running loss:3042.995355, Train batch loss:0.114658, acc:0.946473 \n",
            "Step:  360 2 359: Train running loss:3143.322186, Train batch loss:0.246759, acc:0.946050 \n",
            "Step:  370 2 369: Train running loss:3250.381804, Train batch loss:0.145401, acc:0.945481 \n",
            "Step:  380 2 379: Train running loss:3359.871908, Train batch loss:0.114729, acc:0.945312 \n",
            "Step:  390 2 389: Train running loss:3450.694004, Train batch loss:0.081104, acc:0.945032 \n",
            "Step:  400 2 399: Train running loss:3524.117405, Train batch loss:0.065720, acc:0.945156 \n",
            "Step:  410 2 409: Train running loss:3625.101098, Train batch loss:0.056633, acc:0.945084 \n",
            "Step:  420 2 419: Train running loss:3733.571772, Train batch loss:0.177999, acc:0.944531 \n",
            "Step:  430 2 429: Train running loss:3842.756563, Train batch loss:0.260094, acc:0.944186 \n",
            "Step:  440 2 439: Train running loss:3951.176481, Train batch loss:0.169170, acc:0.943963 \n",
            "Step:  450 2 449: Train running loss:4061.005045, Train batch loss:0.129462, acc:0.943611 \n",
            "Step:  460 2 459: Train running loss:4160.395804, Train batch loss:0.079789, acc:0.943512 \n",
            "Step:  470 2 469: Train running loss:4256.390494, Train batch loss:0.229750, acc:0.943517 \n",
            "Step:  480 2 479: Train running loss:4337.259873, Train batch loss:0.168251, acc:0.943522 \n",
            "Step:  490 2 489: Train running loss:4420.855903, Train batch loss:0.119159, acc:0.943559 \n",
            "Step:  500 2 499: Train running loss:4513.499506, Train batch loss:0.130434, acc:0.943469 \n",
            "2: Train time:1344.70s in 500 steps\n",
            ">> Epoch 3 <<<<<<<<\n",
            "Step:  10 3 9: Train running loss:94.354318, Train batch loss:0.308740, acc:0.942187 \n",
            "Step:  20 3 19: Train running loss:185.238346, Train batch loss:0.227334, acc:0.939063 \n",
            "Step:  30 3 29: Train running loss:290.105024, Train batch loss:0.131507, acc:0.938542 \n",
            "Step:  40 3 39: Train running loss:410.027624, Train batch loss:0.102327, acc:0.937891 \n",
            "Step:  50 3 49: Train running loss:508.589331, Train batch loss:0.097111, acc:0.938125 \n",
            "Step:  60 3 59: Train running loss:605.472646, Train batch loss:0.325537, acc:0.938281 \n",
            "Step:  70 3 69: Train running loss:698.555282, Train batch loss:0.091677, acc:0.938393 \n",
            "Step:  80 3 79: Train running loss:789.252726, Train batch loss:0.084811, acc:0.938086 \n",
            "Step:  90 3 89: Train running loss:881.889533, Train batch loss:0.185834, acc:0.938194 \n",
            "Step:  100 3 99: Train running loss:976.390253, Train batch loss:0.085294, acc:0.938906 \n",
            "Step:  110 3 109: Train running loss:1100.676877, Train batch loss:0.228001, acc:0.937074 \n",
            "Step:  120 3 119: Train running loss:1238.355323, Train batch loss:0.082229, acc:0.935677 \n",
            "Step:  130 3 129: Train running loss:1350.745388, Train batch loss:0.080127, acc:0.935216 \n",
            "Step:  140 3 139: Train running loss:1434.365137, Train batch loss:0.214088, acc:0.936830 \n",
            "Step:  150 3 149: Train running loss:1542.165342, Train batch loss:0.161189, acc:0.935729 \n",
            "Step:  160 3 159: Train running loss:1631.813729, Train batch loss:0.107017, acc:0.935937 \n",
            "Step:  170 3 169: Train running loss:1713.975256, Train batch loss:0.127699, acc:0.937040 \n",
            "Step:  180 3 179: Train running loss:1778.969554, Train batch loss:0.129941, acc:0.938194 \n",
            "Step:  190 3 189: Train running loss:1856.922798, Train batch loss:0.103696, acc:0.938816 \n",
            "Step:  200 3 199: Train running loss:1969.064324, Train batch loss:0.056445, acc:0.938672 \n",
            "Step:  210 3 209: Train running loss:2043.107221, Train batch loss:0.174835, acc:0.939583 \n",
            "Step:  220 3 219: Train running loss:2144.010036, Train batch loss:0.135582, acc:0.939773 \n",
            "Step:  230 3 229: Train running loss:2246.408121, Train batch loss:0.071403, acc:0.939878 \n",
            "Step:  240 3 239: Train running loss:2368.840934, Train batch loss:0.047899, acc:0.939388 \n",
            "Step:  250 3 249: Train running loss:2452.343099, Train batch loss:0.086902, acc:0.939438 \n",
            "Step:  260 3 259: Train running loss:2537.127264, Train batch loss:0.144790, acc:0.939964 \n",
            "Step:  270 3 269: Train running loss:2606.752581, Train batch loss:0.076514, acc:0.940856 \n",
            "Step:  280 3 279: Train running loss:2679.176117, Train batch loss:0.114303, acc:0.941574 \n",
            "Step:  290 3 289: Train running loss:2766.839137, Train batch loss:0.167830, acc:0.941595 \n",
            "Step:  300 3 299: Train running loss:2855.168022, Train batch loss:0.146929, acc:0.941719 \n",
            "Step:  310 3 309: Train running loss:2929.575655, Train batch loss:0.159946, acc:0.941935 \n",
            "Step:  320 3 319: Train running loss:2999.507922, Train batch loss:0.122800, acc:0.942285 \n",
            "Step:  330 3 329: Train running loss:3077.636252, Train batch loss:0.081384, acc:0.942566 \n",
            "Step:  340 3 339: Train running loss:3182.003582, Train batch loss:0.136001, acc:0.942142 \n",
            "Step:  350 3 349: Train running loss:3285.361041, Train batch loss:0.249493, acc:0.942054 \n",
            "Step:  360 3 359: Train running loss:3353.010039, Train batch loss:0.075121, acc:0.942578 \n",
            "Step:  370 3 369: Train running loss:3428.596004, Train batch loss:0.111062, acc:0.942863 \n",
            "Step:  380 3 379: Train running loss:3507.433685, Train batch loss:0.035234, acc:0.942887 \n",
            "Step:  390 3 389: Train running loss:3605.005867, Train batch loss:0.059646, acc:0.942708 \n",
            "Step:  400 3 399: Train running loss:3740.845927, Train batch loss:0.189428, acc:0.942070 \n",
            "Step:  410 3 409: Train running loss:3831.363381, Train batch loss:0.146334, acc:0.942264 \n",
            "Step:  420 3 419: Train running loss:3925.691605, Train batch loss:0.116559, acc:0.942113 \n",
            "Step:  430 3 429: Train running loss:4024.217096, Train batch loss:0.177995, acc:0.941933 \n",
            "Step:  440 3 439: Train running loss:4148.106525, Train batch loss:0.235039, acc:0.941548 \n",
            "Step:  450 3 449: Train running loss:4241.307842, Train batch loss:0.240820, acc:0.941458 \n",
            "Step:  460 3 459: Train running loss:4364.239298, Train batch loss:0.108830, acc:0.940795 \n",
            "Step:  470 3 469: Train running loss:4465.170754, Train batch loss:0.266109, acc:0.940658 \n",
            "Step:  480 3 479: Train running loss:4559.666853, Train batch loss:0.138798, acc:0.940332 \n",
            "Step:  490 3 489: Train running loss:4649.171321, Train batch loss:0.095989, acc:0.940370 \n",
            "Step:  500 3 499: Train running loss:4723.735800, Train batch loss:0.091750, acc:0.940562 \n",
            "3: Train time:1337.18s in 500 steps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09pJ-xumvWDt"
      },
      "source": [
        " for i,params in enumerate(model1.parameters()):\n",
        "  if not params.is_leaf:\n",
        "    print(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw4mB0dip6Yf"
      },
      "source": [
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model1.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            'running_loss': running_loss_list,\n",
        "            'accuracy': acc_list,\n",
        "            'total_epochs':epochs\n",
        "            }, 'MODELS/SAGEConv_trial1_0-1-2-3-4_channels_256-512-1024-layers_with_SAGPooling+readout.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MyI8Vq4LFeM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6b5351-3e4c-4267-d46c-60d4bfbcc647"
      },
      "source": [
        "BATCH_SZ = 64\n",
        "validDecays = glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_64000_to_70399.parquet')\n",
        "validDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_70400_to_76799.parquet'))\n",
        "validDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_76800_to_83199.parquet'))\n",
        "validDecays.extend(glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_83200_to_89599.parquet'))\n",
        "#validDecays = glob.glob('./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_12800_to_19199.parquet')\n",
        "assert len(validDecays) == 4, \"len(decays) = %d\"%(len(validDecays))\n",
        "\n",
        "print(\">> Input files:\",validDecays)\n",
        "dset_valid = ConcatDataset([TrainParquetDataset(d) for d in validDecays])\n",
        "idxs = np.random.permutation(6400)\n",
        "valid_sampler = sampler.SubsetRandomSampler(idxs[:6400])\n",
        "valid_loader = DataLoader(dataset=dset_valid, batch_size=BATCH_SZ, num_workers=0, sampler=valid_sampler, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">> Input files: ['./data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_64000_to_70399.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_70400_to_76799.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_76800_to_83199.parquet', './data_shared/BoostedTopParquet_x1_fixed_Graphs/BoostedTop_x1_train_samples_83200_to_89599.parquet']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDhwAvH7gLS0"
      },
      "source": [
        "import gc \n",
        "gc.collect() \n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiYUdZDrR3Up"
      },
      "source": [
        "## Validation & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AM0hl34ovYna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc7f217b-c9a2-49c2-8975-500b94631bf9"
      },
      "source": [
        "import gc \n",
        "\n",
        "USE_PREVIOUS = True\n",
        "\n",
        "if (USE_PREVIOUS):\n",
        "  model1 = PointNet(num_node_features)\n",
        "  model1.cuda()\n",
        "  #model1 = torch.nn.DataParallel(model1, device_ids=[0])\n",
        "  optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
        "  lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10,20], gamma=0.5)\n",
        "\n",
        "  checkpoint = torch.load('MODELS/SAGEConv_trial1_0-1-2-3-4_channels_256-512-1024-layers_without_SAGPooling+readout.pt')\n",
        "  model1.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  start_epoch = checkpoint['epoch']\n",
        "  loss = checkpoint['loss']\n",
        "  running_loss_list = checkpoint['running_loss']\n",
        "  acc_list = checkpoint['accuracy']\n",
        "  epochs = checkpoint['total_epochs']\n",
        "\n",
        "  print('Prevsious durring Trainng:')\n",
        "  print(' >> Epochs finished earlier : ',start_epoch,' out of ',epochs,'. Continuing remaining training.')\n",
        "  print(' >> Batch Loss: ', loss)\n",
        "  print(' >> Running Loss: ',running_loss_list)\n",
        "  print(' >> Accuracy: ',acc_list )\n",
        "  print(' >> Optimizer: ',optimizer)\n",
        "  print(' >> lr_scheduler: ',lr_scheduler)\n",
        "\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Validation\n",
        "batch_data=[]\n",
        "true_y=[]\n",
        "pred_list=[]\n",
        "val_y_list = []\n",
        "logits_list = []\n",
        "acc_list = []\n",
        "sum_list=[]\n",
        "BATCH_SZ = 64\n",
        "val_step=0\n",
        "val_max_samples=6400#3200#valid_sz\n",
        "print_step=10\n",
        "running_loss, acc_ = 0., 0.\n",
        "c1,c2=0, BATCH_SZ\n",
        "model1.eval()\n",
        "\n",
        "for i, data in enumerate(valid_loader):\n",
        "  \n",
        "  if i>val_max_samples:\n",
        "    break\n",
        "  #data.append(data)\n",
        "  #true_y.append(int(data['y'].item()))\n",
        "  #if len(data)<BATCH_SZ:  \n",
        "  #  continue\n",
        "  \n",
        "  val_step+=1\n",
        "  print(\"Step: \",val_step, i, len(data['y']))\n",
        "  if len(data['y'])<c2 or len(data['y'])<BATCH_SZ:\n",
        "    print(\" >> Last Batch with less samples than Batch size: Found \", len(data['y']), \"samples but Batch size is \",c2)\n",
        "    c2 = len(data['y'])\n",
        "  else:\n",
        "    c2 = BATCH_SZ\n",
        "        \n",
        "        \n",
        "  rawGraph = graphCloud(data,c1,c2) ##Generating graphs from raw data\n",
        "  nodeCount = nodeCounter(rawGraph)\n",
        "  lengs=torch.LongTensor(np.hstack(assigner(np.array(nodeCount[c1:c2])))).cuda()\n",
        "\n",
        "  compress = torch_geometric.data.Batch.from_data_list(rawGraph)\n",
        "\n",
        "  gra = compress.x.clone()\n",
        "  pos = compress.pos.clone()\n",
        "  adj = compress.edge_index.clone()\n",
        "  gra.cuda()\n",
        "  pos.cuda()\n",
        "  adj.cuda()\n",
        "  lengs.cuda()\n",
        "\n",
        "  #model1.zero_grad()\n",
        "  #print('conv1.bias.grad before backward')\n",
        "  #print(model1.edgeConv0.nn.nn[0].bias)\n",
        "  #optimizer.zero_grad()\n",
        "  gc.collect() \n",
        "  torch.cuda.empty_cache()\n",
        "  true_y = torch.squeeze(data['y'])\n",
        "  logits, stage1_graph = model1(gra,lengs,adj,pos,nodeCount)\n",
        "  #logits_list.extend((logits[:,1].to('cpu')).detach().numpy().tolist())\n",
        "  logits_list.extend((logits.to('cpu')).detach().numpy().tolist())\n",
        "  loss = torch.nn.BCELoss()(torch.squeeze(logits), true_y.to('cuda'))#F.cross_entropy(logits, torch.tensor(true_y).to('cuda'))\n",
        "  \n",
        "  #print('conv1.bias.grad after backward')\n",
        "  #print(model1.edgeConv0.nn.nn[0].bias)\n",
        "\n",
        "  running_loss += loss.item()*c2\n",
        "  #pred = torch.argmax(logits, dim=1)\n",
        "  pred = torch.zeros_like(logits)\n",
        "  pred[logits>0.5] = 1\n",
        "  pred = torch.squeeze(pred)\n",
        "  pred_list.extend(np.array(pred.to('cpu')).tolist())\n",
        "  val_y_list.extend(true_y)\n",
        "  acc = sum(pred==torch.tensor(true_y).to('cuda')).item()\n",
        "  sum_list.append(acc)\n",
        "  acc_ = (acc_*BATCH_SZ*(val_step-1) + acc)/(c2+BATCH_SZ*(val_step-1))\n",
        "  #print('epoch: ',epoch, 'sample: ',i, 'loss: ',loss.item(),'running loss', running_loss, 'correct predictions in batch: ',acc, 'accuracy: ',acc_)\n",
        "  #print('sample no: ',i,'epoch no: ',e)\n",
        "  if val_step % print_step == 0:\n",
        "        #torch.save(model1.state_dict(),'MODELS/PointNet_model1_lr_1e-3_epochs_'+str(epoch)+'_Batch_'+str(BATCH_SZ)+'_100*3200_samples.pt')\n",
        "    s = '%d: Valid running loss:%f, Valid batch loss:%f, acc:%f '%(i, running_loss, loss.item(), acc_)\n",
        "    print(s)\n",
        "    print(len(pred_list),len(val_y_list))\n",
        "  val_data, val_y = data, true_y\n",
        "  batch_data=[]  # Flush batch data\n",
        "  true_y=[]\n",
        "  gc.collect() \n",
        "  torch.cuda.empty_cache()\n",
        "    # For more frequent validation:\n",
        "    #if epoch > 1 and i % eval_step == 0:\n",
        "    #    model1.eval()\n",
        "    #    roc_auc_best = do_eval(model1, val_loader, f, roc_auc_best, epoch)\n",
        "    #    model1.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prevsious durring Trainng:\n",
            " >> Epochs finished earlier :  3  out of  3 . Continuing remaining training.\n",
            " >> Batch Loss:  tensor(0.0918, device='cuda:0', requires_grad=True)\n",
            " >> Running Loss:  [13298.006275177002, 11620.754623413086, 11082.379445075989, 10609.699986457825, 9949.00496673584, 10021.686545848846, 9608.798424720764, 8946.212854385376, 8711.667983055115, 8098.950366973877, 8372.715818405151, 8123.056043148041, 7942.492605686188, 7657.1106133461, 7662.134208202362, 7446.212458610535, 7239.637764215469, 7205.474262475967, 7045.145822763443, 7007.376295089722, 6833.058912754059, 6762.9165551662445, 6695.367338418961, 6490.576879739761, 6353.290660381317, 6276.783454656601, 6253.090657711029, 5768.962197065353, 6076.603927373886, 5854.505924701691, 5308.150457620621, 5609.639824390411, 5435.311896204948, 4969.168320417404, 4864.724485635757, 4955.576825618744, 4513.499505996704, 4723.735799789429]\n",
            " >> Accuracy:  [0.81965625, 0.8483437499999997, 0.85609375, 0.8620937499999999, 0.8709687500000001, 0.8713750000000002, 0.8767812499999998, 0.885125, 0.8910312500000003, 0.898375, 0.89525, 0.89834375, 0.90109375, 0.903625, 0.9033125, 0.90625, 0.91084375, 0.9090625, 0.91171875, 0.9137500000000001, 0.9144999999999999, 0.9165625, 0.9158125, 0.919, 0.92159375, 0.920625, 0.92225, 0.927875, 0.9241875, 0.92871875, 0.9345625, 0.92996875, 0.93275, 0.9385625, 0.9383125, 0.93884375, 0.94346875, 0.9405625]\n",
            " >> Optimizer:  Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    weight_decay: 0\n",
            ")\n",
            " >> lr_scheduler:  <torch.optim.lr_scheduler.MultiStepLR object at 0x7fc1c0bd1790>\n",
            "Step:  1 0 64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:101: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Step:  2 1 64\n",
            "Step:  3 2 64\n",
            "Step:  4 3 64\n",
            "Step:  5 4 64\n",
            "Step:  6 5 64\n",
            "Step:  7 6 64\n",
            "Step:  8 7 64\n",
            "Step:  9 8 64\n",
            "Step:  10 9 64\n",
            "9: Valid running loss:147.497521, Valid batch loss:0.186326, acc:0.909375 \n",
            "640 640\n",
            "Step:  11 10 64\n",
            "Step:  12 11 64\n",
            "Step:  13 12 64\n",
            "Step:  14 13 64\n",
            "Step:  15 14 64\n",
            "Step:  16 15 64\n",
            "Step:  17 16 64\n",
            "Step:  18 17 64\n",
            "Step:  19 18 64\n",
            "Step:  20 19 64\n",
            "19: Valid running loss:294.497120, Valid batch loss:0.204486, acc:0.915625 \n",
            "1280 1280\n",
            "Step:  21 20 64\n",
            "Step:  22 21 64\n",
            "Step:  23 22 64\n",
            "Step:  24 23 64\n",
            "Step:  25 24 64\n",
            "Step:  26 25 64\n",
            "Step:  27 26 64\n",
            "Step:  28 27 64\n",
            "Step:  29 28 64\n",
            "Step:  30 29 64\n",
            "29: Valid running loss:450.205644, Valid batch loss:0.249925, acc:0.910937 \n",
            "1920 1920\n",
            "Step:  31 30 64\n",
            "Step:  32 31 64\n",
            "Step:  33 32 64\n",
            "Step:  34 33 64\n",
            "Step:  35 34 64\n",
            "Step:  36 35 64\n",
            "Step:  37 36 64\n",
            "Step:  38 37 64\n",
            "Step:  39 38 64\n",
            "Step:  40 39 64\n",
            "39: Valid running loss:599.851555, Valid batch loss:0.366780, acc:0.911328 \n",
            "2560 2560\n",
            "Step:  41 40 64\n",
            "Step:  42 41 64\n",
            "Step:  43 42 64\n",
            "Step:  44 43 64\n",
            "Step:  45 44 64\n",
            "Step:  46 45 64\n",
            "Step:  47 46 64\n",
            "Step:  48 47 64\n",
            "Step:  49 48 64\n",
            "Step:  50 49 64\n",
            "49: Valid running loss:743.087140, Valid batch loss:0.307293, acc:0.914375 \n",
            "3200 3200\n",
            "Step:  51 50 64\n",
            "Step:  52 51 64\n",
            "Step:  53 52 64\n",
            "Step:  54 53 64\n",
            "Step:  55 54 64\n",
            "Step:  56 55 64\n",
            "Step:  57 56 64\n",
            "Step:  58 57 64\n",
            "Step:  59 58 64\n",
            "Step:  60 59 64\n",
            "59: Valid running loss:896.531221, Valid batch loss:0.070596, acc:0.913542 \n",
            "3840 3840\n",
            "Step:  61 60 64\n",
            "Step:  62 61 64\n",
            "Step:  63 62 64\n",
            "Step:  64 63 64\n",
            "Step:  65 64 64\n",
            "Step:  66 65 64\n",
            "Step:  67 66 64\n",
            "Step:  68 67 64\n",
            "Step:  69 68 64\n",
            "Step:  70 69 64\n",
            "69: Valid running loss:1050.918261, Valid batch loss:0.140966, acc:0.913616 \n",
            "4480 4480\n",
            "Step:  71 70 64\n",
            "Step:  72 71 64\n",
            "Step:  73 72 64\n",
            "Step:  74 73 64\n",
            "Step:  75 74 64\n",
            "Step:  76 75 64\n",
            "Step:  77 76 64\n",
            "Step:  78 77 64\n",
            "Step:  79 78 64\n",
            "Step:  80 79 64\n",
            "79: Valid running loss:1221.270927, Valid batch loss:0.281620, acc:0.910156 \n",
            "5120 5120\n",
            "Step:  81 80 64\n",
            "Step:  82 81 64\n",
            "Step:  83 82 64\n",
            "Step:  84 83 64\n",
            "Step:  85 84 64\n",
            "Step:  86 85 64\n",
            "Step:  87 86 64\n",
            "Step:  88 87 64\n",
            "Step:  89 88 64\n",
            "Step:  90 89 64\n",
            "89: Valid running loss:1383.198620, Valid batch loss:0.424873, acc:0.909028 \n",
            "5760 5760\n",
            "Step:  91 90 64\n",
            "Step:  92 91 64\n",
            "Step:  93 92 64\n",
            "Step:  94 93 64\n",
            "Step:  95 94 64\n",
            "Step:  96 95 64\n",
            "Step:  97 96 64\n",
            "Step:  98 97 64\n",
            "Step:  99 98 64\n",
            "Step:  100 99 64\n",
            "99: Valid running loss:1532.779168, Valid batch loss:0.123416, acc:0.910937 \n",
            "6400 6400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bglPkegR8Eq"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz2xkkll6TWM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "ad975ac6-9df0-4ea2-8244-827a60ab96cf"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "fpr,tpr,_ = roc_curve(np.array(val_y_list),np.array(logits_list))\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print('Test AUC: ' + str(roc_auc))\n",
        "\n",
        "pyplot.plot(fpr, tpr, marker='.', label='GNN')\n",
        "# axis labels\n",
        "pyplot.xlabel('False Positive Rate')\n",
        "pyplot.ylabel('True Positive Rate')\n",
        "# show the legend\n",
        "pyplot.legend()\n",
        "# show the plot\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test AUC: 0.9704964142470395\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3hV1Z3/8fc3ITGIXDSgcpGbghWRoqSAlypqtaIW9anjhTqtU1trq870p3XGqfxsa9XS2nY6Tp1Wqjy2HSnaduTBCvpTKyoqCBFELsViJBAugjEgcg3k+/tj74STk5OTE5J9Tk725/U85+GsvdfZ+7sDnG/WWnuvZe6OiIjEV0GuAxARkdxSIhARiTklAhGRmFMiEBGJOSUCEZGY65LrAFqrd+/ePnjw4FyHISKSV8rLyz909z6p9uVdIhg8eDCLFy/OdRgiInnFzCqb26euIRGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZiLLBGY2XQz22Jmy5vZb2b2oJmtMbNlZnZaVLGIiEjzomwRPAZclGb/RGBY+LoR+FWEsYg0Ul5Zw0MvraG8siaj+jMWruMfH13IjIXr2q1ua46Z6THa45jSMbX232xrRPYcgbu/YmaD01S5DPidB/NgLzCzXmbW1903RRWTRKu8soYFFdUsrKhm2YbtDD7qcD7Zu59te2rp1bWYkf16sHzDdrbtqQVo2Fa9cx+l3Yqp3rmPiSP7AjB3+aaG99PnV4AZXz1zCJPHDQSCL7z6OvXbEs1YUMkj8ysA4+rPHEd5ZQ1vvFdN2eBeHNXtMP73rQ04UFRo3HPZSIYf0x1w6hzq6hwH6tzB4YVVHzD9tbUAvPr3D1m+YRtnDetDsDuYxj2symtrPuSJResb6i5dX8MZx/fGcepnfHeH1ys+5M/lGxrqlVd+xLghpY2OB8ExD773RtsXvV/N7Lc3NRzjf95Yy8rNOxrKL6/eyuiBvRp+JvXHbkkUM9NnOt19puduTYiZHzN3MbZ00E3bd/PUko3UuVPcpYDHvzaeMYOObM0Z0rIo1yMIE8Ff3H1kin1/Aaa6+/yw/CLwb+7e5GkxM7uRoNXAwIEDx1RWNvtchLTB1DmreHLxeurc6dLlYGNx974D1NVBabciqnfuo0uBcXSPEoYf252PPtnHtt37WF+9m921B1r3j/8QHF5ciLuzu7auYVtRoVFYYMGXscP+A3XUpTmGSEdk1vy+xK/pQoPbLjyRm889oZXHt3J3L0u1Ly+eLHb3acA0gLKyMq2kcwi+/OhCXn/vw7BkHNWtiH49u7KsanvGX5pV2w40vN+xdSfvbd3Z7nG2pGfXIgzYvX1Pw7ajux/GJaP6YQYFZvzl7Y2sr9md8TEnnNib688YQoEZBWaYBf8pDaPA4MVVW5j2akVD/X8+7wQuGdUPqK9Hw/tnlm3iP174e0Pd2y4Yxhc+3T/Yz8H/7E8v3chPn3+3od6/fn44k0b3x8waHS/4nCW8p+GET721gR/N/VvDMb4w6lieXra5ofyDSSO4+jNNW0uZSPel1KQumVXO9JiZntpaEWTmx2z/c7eH8soavvTIAmr311HUpYDxQ0vb9fi5TAQbgOMSygPCbXKIyitrmPLUO6wKuwcACgugri65meps2bGPLTv2ZTvENrv1vGEAfPepdxq23XzusEbdQ8cdeXij/ekUFsCt5w1P28weN7SUwb27pe2Kqvcvn+tOn+4lLda95fxhHHXEYRkdsznfOOd4upcUNTrG6cen7zKT/DRm0JE8/rXxLKioZvzQ0nbtFoLcdg1dAtwCXAyMAx5097EtHbOsrMw111Dwpf/rl9/j1Xe3smd/x+sIOaxLAScd2z13YwQL1zX63JvvVzPv3a1MGN6HsUNKeWLROo7pUcI3zjm+3f9TiXRE6bqGIksEZvYHYALQG/gA+B5QBODuv7agbfVLgjuLdgH/lGp8IFlcE8HUOat4dH4FtVn6zu/VtUvDOEGqMYJjenZt+BLfsbuWtR/tok+3YmrrnItOPpY7Lz4pO4GKSEZyMkbg7te2sN+Bm6M6f2cx8u5n+WTfgZYrtkJhAZR2K240RlBocOYJvRk3tDSSpqeIdFx5MVgcN+355V9YACVFhdTud044uhs/vPwUfcmLSCNKBB3A1Dmr+N0ba9l7oI4Dbez6KSwwvn7WEHXNiEjGlAhy7KQpc9l9iIO9BpzQpxvP3z6hXWMSkXhRIsiBVLd5ZqK0WxHTvvwZde2ISLtSIsiiqXNW8fArFa1++rZvj8P45ZfGKAGISCSUCLLgUBJAcWFw/7v6+kUkakoEETvhu8+Q6RDA5aP78YtrTo02IBGRJEoEESmvrOGLv3o9o7olXQp4/OvtO5ugiEimlAgikOmdQMN0x4+IdABKBO1s8J3PtFjnprOHqu9fRDoMJYJ2dMHP5qXd37VLAavunZidYEREMqRE0I7+nmZ+frUCRKSjUiJoB1PnrOLXr1Sk3DegVwnz7zw/yxGJiGROiaCNvj1zCbOWbky5b9BRh/Pyv56b5YhERFqnoOUq0pzyyppmkwDAz68encVoREQOjRJBG6R7TuD+KzTds4jkB3UNHaKzpr7Y7L4/f/MMJQERyRtKBIfg2zOXULVtT8p9a6dekuVoRETaRomglUb/4Dm27d6fct+fv3lGlqMREWk7jRG0wllTX2w2CQzoVaLuIBHJS0oEGSqvrGm2O6gA9KyAiOQtJYIM3fJ4ecrtBUCFxgVEJI8pEWRo08d7m2zrUqAkICL5T4kgAydNmZty+5r7lQREJP8pEbRg6pxVKdcWOLxIPzoR6Rz0bdaCh5uZTO7Lpw/ObiAiIhFRIkjjgp/NS7ng/OgBPTWltIh0GkoEaaRaX6DQYNYtZ+UgGhGRaCgRNGPqnFUpt3/9s0OzHImISLSUCJrxZHlVk22FhrqERKTTUSJoRtcUdwWpNSAinVGkicDMLjKz1Wa2xszuTLF/oJm9ZGZLzGyZmV0cZTytYUnlI4oL1RoQkU4pskRgZoXAQ8BEYARwrZmNSKo2BXjS3U8FrgH+O6p4WmtD0rxCu2oP5CgSEZFoRdkiGAuscfcKd98HzAQuS6rjQI/wfU+g+XUfs2jqnFVNbhstLlQvmoh0TlF+u/UH1ieUq8Jtib4PXGdmVcAc4NZUBzKzG81ssZkt3rp1axSxNvL7BZVNtn12eJ/Izysikgu5/jX3WuAxdx8AXAz83syaxOTu09y9zN3L+vSJ/gt5176m3UA3nXN85OcVEcmFKBPBBuC4hPKAcFuiG4AnAdz9DaAE6B1hTIfEQIvOiEinFWUiWAQMM7MhZlZMMBg8O6nOOuB8ADM7iSARRN/3k0Z5ZU2T8YEeXbWip4h0XpElAnffD9wCPAesIrg7aIWZ3WNmk8JqtwNfN7O3gT8A17t7qul9sub/znqnybaxQ0pzEImISHZE+quuu88hGARO3HZ3wvuVwJlRxtBaqz/Y0WSbxgdEpDPL9WBxh3Og6dIDGh8QkU5NiSBBqonmCpMfMRYR6WSUCBI8u2Jzk23Dj+meg0hERLJHiSBBUUHTX//vveKUHEQiIpI9SgQJ1tXsblQuLizQ+ICIdHpKBKHyyhr2Ji1SX+cpRo5FRDoZJYLQLY+X5zoEEZGcUCIIbf54b5NtZxzf4Wa7EBFpd0oEoeQVyQqA390wLjfBiIhkkRJBaNLofo3KultIROIi40RgZodHGUgulVfW8OTig4vVFxbAicfq+QERiYcWE4GZnWFmK4G/heVPm1mHWVKyPTz88nvUJUx1d6AOFlRU5y4gEZEsyqRF8B/A54FqAHd/Gzg7yqCy7YOPG69PbAbjh2rGURGJh4y6htx9fdKmTrWS+9WfGdio/I3PDtWDZCISG5kkgvVmdgbgZlZkZt8hWF+g05g8biCXnnIsAFMuOYk7Lz4pxxGJiGRPJongJuBmgoXnNwCjgW9FGVQuvLWuBoBZS6paqCki0rlkkghOdPcvufsx7n60u18HdKpfmS//5Xw2bg8eKFu+cQeX/3J+jiMSEcmeTBLBf2W4LW+9s2F7o/LyjR/nKBIRkexrdqlKMzsdOAPoY2a3JezqARRGHVi2lFfWcCBpleSSLnrOTkTiI92axcXAEWGdxKerPgaujDKobHr45feabNvvnqKmiEjn1GwicPeXgZfN7DF3r8xiTFmV/AwBwNjBR+UgEhGR3EjXIqi3y8weAE4GSuo3uvt5kUWVRUN6d+PtqoNjBH2OKNZkcyISK5l0hj9OML3EEOAHwFpgUYQxZVX1zn2Nyp/q2yNHkYiI5EYmiaDU3R8Fat39ZXf/KtApWgMAE0f2TVsWEensMukaqg3/3GRmlwAbAXWii4h0Epm0CO41s57A7cB3gEeAb0caVRb94oXVacsiIp1diy0Cd/9L+HY7cC6AmZ0ZZVDZ9FHSGMFHO2ubqSki0jmle6CsELiKYI6hZ919uZldCnwX6Aqcmp0Qo3VYUSH79x6cTLXX4UU5jEZEJPvStQgeBY4D3gQeNLONQBlwp7vPykZwUSuvrGHX3sYzal952oAcRSMikhvpEkEZMMrd68ysBNgMHO/unWbprgUV1SQ/Q9y9q1oEIhIv6QaL97l7HYC77wEqWpsEzOwiM1ttZmvM7M5m6lxlZivNbIWZzWjN8dtq/NDSRj+A4i4FWplMRGInXYvgU2a2LHxvwPFh2QB391HpDhyOMTwEXABUAYvMbLa7r0yoMwz4d+BMd68xs6PbcC2ttnrzDuoSyl89Y7BWJhOR2EmXCNq65sBYYI27VwCY2UzgMmBlQp2vAw+5ew2Au29p4zlbZe7yTY3KKzZp+mkRiZ90k861daK5/kDiWsdVQPIkPsMBzOw1gqmtv+/uzyYfyMxuBG4EGDhwYPLuQzZxZF9e/fuHjcoiInGT64n3uwDDgAnAtcBvzKxXciV3n+buZe5e1qdPn3Y7+eRxAznx2CMAuHx0PyaPa78kIyKSL6JMBBsIbj+tNyDclqgKmO3ute7+PvAuQWLIihkL17F68ycAzFq6kRkL12Xr1CIiHUZGicDMuprZia089iJgmJkNMbNi4BpgdlKdWQStAcysN0FXUUUrz3PIpr/2ftqyiEgctJgIzOwLwFLg2bA82sySv9CbcPf9wC3Ac8Aq4El3X2Fm95jZpLDac0C1ma0EXgLuyOZzCtt270tbFhGJg0xmH/0+wR1A8wDcfamZDcnk4O4+B5iTtO3uhPcO3Ba+sq5XSREf7tjXqCwiEjeZdA3Vuvv2pG2dYlHfvj1LGpVH9u+Zo0hERHInkxbBCjObDBSGD4D9M/B6tGFFr7yyhlfXNO6FWrz2oxxFIyKSO5m0CG4lWK94LzCDYDrqvF+PYEFF06GIXfsOpKgpItK5ZdIi+JS73wXcFXUw2bRjd9N1B64qOy5FTRGRzi2TFsHPzGyVmf3QzEZGHlGWJE8n0b9XCXde3NZZNURE8k+LicDdzyVYmWwr8LCZvWNmUyKPLGLJ00ncfG7WnmMTEelQMnqgzN03u/uDwE0EzxTc3cJHREQkT2TyQNlJZvZ9M3sH+C+CO4byfhmv5JlHk8siInGRyWDxdOAJ4PPuvjHieLKmtFtx2rKISFy0mAjc/fRsBJJtyzd+nLYsIhIXzSYCM3vS3a8Ku4QSnyTOaIWyjm73vv2NN3ineFhaRKTV0rUI/iX889JsBJJN5ZU1bNi2p9G2fr265igaEZHcanaw2N3rR0+/5e6ViS/gW9kJLxoPv/xek23rPtqVg0hERHIvk9tHL0ixbWJ7B5JNqdYmvujkY3MQiYhI7qUbI/gmwW/+Q81sWcKu7sBrUQcWpa5dGue/nl276KliEYmtdGMEM4C5wI+AOxO273D3vJ6mc2T/nqzZurOhfO6JR+cwGhGR3EqXCNzd15rZzck7zOyofE4GunVUROSglloElwLlBLePWsI+B4ZGGFe0km8V1a2jIhJjzSYCd780/DOjZSnzSXLXkFYmE5E4y2SuoTPNrFv4/joz+7mZDYw+tOhU79yXtiwiEieZ3D76K2CXmX0auB14D/h9pFFFLHlRmlSL1IiIxEUmiWC/uztwGfBLd3+I4BbSvLUiaXB4rR4mE5EYy2T20R1m9u/APwKfNbMCoCjasKLz7ZlLqK1rPDg8SmMEIhJjmbQIriZYuP6r7r6ZYC2CByKNKkLz3t3aZNu4oaU5iEREpGPIZKnKzcDjQE8zuxTY4+6/izyyiAw+6vBGZTMYr0QgIjGWyV1DVwFvAv8AXAUsNLMrow4sKt27Nu7VGtW/J2MGHZmjaEREci+TMYK7gM+4+xYAM+sDvAD8KcrAojJxZF9e/fuHDeWrP5PXd8KKiLRZJmMEBfVJIFSd4ec6pMnjBjK0d9A9dPnofkwep0QgIvGWyRf6s2b2nJldb2bXA88Ac6INKzozFq6j4sPgdtFZSzcyY+G6HEckIpJbmQwW3wE8DIwKX9Pc/d+iDiwqv3hhddqyiEjcpFuPYBjwU+B44B3gO+6+IVuBReWjnbVpyyIicZOuRTAd+AvwRYIZSP+rtQc3s4vMbLWZrTGzO9PU+6KZuZmVtfYcrdXr8KK0ZRGRuEmXCLq7+2/cfbW7/xQY3JoDm1kh8BDBspYjgGvNbESKet2BfwEWtub4h+qsE3qnLYuIxE2620dLzOxUDq5D0DWx7O5vtXDsscAad68AMLOZBPMVrUyq90Pgx8AdrYz9kGz+eE+jsmYeFZG4S5cINgE/TyhvTig7cF4Lx+4PrE8oVwHjEiuY2WnAce7+jJk1mwjM7EbgRoCBA9t2u2dxYeNGUGm34jYdT0Qk36VbmObcKE8cTl73c+D6luq6+zRgGkBZWVmblhPbtqvx4LBaBCISd1E+GLYBOC6hPCDcVq87MBKYZ2ZrgfHA7KgHjMcNPapReeLIvlGeTkSkw8tkiolDtQgYZmZDCBLANcDk+p3uvh1oGKk1s3kEt6gujjAmBoaTzh3VrYirxhynJ4tFJPYiaxG4+37gFuA5YBXwpLuvMLN7zGxSVOdNp7yyhh88HYxVf7Szlumvr6W8siYXoYiIdBgttgjMzIAvAUPd/Z5wveJj3f3Nlj7r7nNImo7C3e9upu6EjCJugwUV1exPWJSmdn8dCyqqNfuoiMRaJi2C/wZOB64NyzsIng/IO8lrExcUmNYiEJHYy2SMYJy7n2ZmSwDcvcbM8vKeyxWbGq9VPLJfD7UGRCT2MmkR1IZPCTs0rEdQF2lUEUm+Q0hrEYiIZJYIHgSeAo42s/uA+cD9kUYVkcnjBjL86G6A1iIQEanXYteQuz9uZuXA+QTTS1zu7qsijywCMxau490tO4FgLYKxQ0qVDEQk9jJZs3ggsAt4GpgN7Ay35Z3pr72ftiwiEkeZDBY/QzA+YEAJMARYDZwcYVzRcE9fFhGJoUxWKDvF3UeFfw4jmFX0jehDa39fPWto2rKISBy1+snicPrpcS1W7IAmjxtIv54lHNmtiPuvOEXjAyIiZPZk8W0JxQLgNGBjZBFFrE7dQSIijWQyRtA94f1+gjGDP0cTTrRmLFzH5o/3AvDdp94BUKtARGIvbSIIHyTr7u7fyVI8kbr/mcaLo/3ihdVKBCISe82OEZhZF3c/AJyZxXgiM2PhOj7Zd6DRtuRFakRE4ihdi+BNgvGApWY2G/gjsLN+p7v/b8SxtasnFq1rsu34PkfkIBIRkY4lkzGCEqCaYI3i+ucJHMirRHB0jxJge6Nt915xSm6CERHpQNIlgqPDO4aWczAB1Mu7W29uOud4nl/5QUP5/itO0cyjIiKkf46gEDgifHVPeF//yiu/f2Nto/Kb71fnJA4RkY4mXYtgk7vfk7VIIjbv3a1pyyIicZWuRWBp9uWdweGi9c2VRUTiKl0iOD9rUWRB8q2jyWURkbhqNhG4+0fZDCRymnlURCSlVk86l69G9u+ZtiwiElexSQTLN36ctiwiElexSQTqGhIRSS02iWBP7YG0ZRGRuIpNIqiffrq5sohIXMUmERxxWGHasohIXMUmEexMem4guSwiElexSQTJS1S6BotFRIAYJYJzhvVuVL50VL8cRSIi0rFEmgjM7CIzW21ma8zszhT7bzOzlWa2zMxeNLNBUcUysn+vRuWxQ0qjOpWISF6JLBGE6x0/BEwERgDXmtmIpGpLgDJ3HwX8CfhJVPHMWrKhUXn6a+9HdSoRkbwSZYtgLLDG3SvcfR8wE7gssYK7v+Tuu8LiAmBAVMHs2Nt4feJtu/dFdSoRkbwSZSLoD6xPKFeF25pzAzA31Q4zu9HMFpvZ4q1bD20dgb21dY3LumtIRAToIIPFZnYdUAY8kGq/u09z9zJ3L+vTp88hnWNXUiL4ZK8SgYgIZLZ4/aHaAByXUB4QbmvEzD4H3AWc4+5Ze9xXN4+KiASibBEsAoaZ2RAzKwauAWYnVjCzU4GHgUnuviXCWDjxmG6NyqMHaBpqERGIMBG4+37gFuA5YBXwpLuvMLN7zGxSWO0B4Ajgj2a21MxmN3O4Nhs14ODto8P6dGPWLWdFdSoRkbwSZdcQ7j4HmJO07e6E95+L8vz1ps5ZxR/LD/ZKVVTvoryyhjGDjszG6UVEOrQOMVgctRlvrmtUPlDnLKiozlE0IiIdSywSwc69+5tsGz9UTxaLiEBMEsGBFLcIqVtIRCQQi0SQfJGxuGgRkQzF4jvx6O6HpS2LiMRZLBLB3gMH0pZFROIsFolgT9L0EsllEZE4i0Ui+PzJx6Yti4jEWSwSwS+uOZVjewTjAqf0684vrjk1xxGJiHQcsUgEMxauY/PHwXx272zcwYyF61r4hIhIfMQiEcxdviltWUQkzmKRCEq7Facti4jEWSwSwbx3t6Yti4jEWSwSwbZdtWnLIiJxFotEICIizYtFIigqSF8WEYmzWHwlOpa2LCISZ7FIBE2XqtfS9SIi9WKRCOrq0pdFROIsFolARESaF4tEkNwAUINAROSgLrkOIBsKrfFylYUaKxaJndraWqqqqtizZ0+uQ4lUSUkJAwYMoKioKOPPxCIRuKcvi0jnV1VVRffu3Rk8eDBmnfO3QXenurqaqqoqhgwZkvHn1DUkIrGwZ88eSktLO20SADAzSktLW93qiUUiEBEBOnUSqHco16hEICISc0oEIiJZ9MEHHzB58mSGDh3KmDFjOP3003nqqaeYN28eZsbTTz/dUPfSSy9l3rx5AEyYMIGysrKGfYsXL2bChAntEpMSgYhIM8ora3jopTWUV9a0y/Hcncsvv5yzzz6biooKysvLmTlzJlVVVQAMGDCA++67r9nPb9myhblz57ZLLIlicdeQiEiiHzy9gpUbP05bZ8eeWv62eQd1DgUGnzq2O91Lmr8lc0S/HnzvCyenPeZf//pXiouLuemmmxq2DRo0iFtvvZV58+bx6U9/mtraWp5//nkuuOCCJp+/4447uO+++5g4cWILV9g6ahGIiKTw8Z791IW3mtd5UG6rFStWcNppp6Wtc9ddd3Hvvfem3Hf66adTXFzMSy+91OZYEqlFICKx09Jv7hB0C33pkQXU7q+jqEsB/3nNqYwZdGS7xnHzzTczf/58iouLeeCBBwA4++yzAZg/f37Kz0yZMoV7772XH//4x+0WR6QtAjO7yMxWm9kaM7szxf7DzOyJcP9CMxscZTwiIpkaM+hIHv/aeG678EQe/9r4dkkCJ598Mm+99VZD+aGHHuLFF19k69bGy+emaxWcd9557N69mwULFrQ5nnqRJQIzKwQeAiYCI4BrzWxEUrUbgBp3PwH4D6D9UpyISBuNGXQkN597Qru1BM477zz27NnDr371q4Ztu3btalLvwgsvpKamhmXLlqU8zpQpU/jJT37SLjFBtC2CscAad69w933ATOCypDqXAb8N3/8JON/i8MSHiMSSmTFr1ixefvllhgwZwtixY/nKV76SspvnrrvuYv369SmPc/HFF9OnT592iyvKMYL+QOJVVAHjmqvj7vvNbDtQCnyYWMnMbgRuBBg4cGCrAynpUsCe/XWNyiIiudC3b19mzpyZcl/icwGTJk3CEyZGq3+eoF55eXm7xZQX34juPs3dy9y97FCy4N1JA0PJZRGROIuyRbABOC6hPCDclqpOlZl1AXoC1e0dyORxQSti7vJNTBzZt6EsIiLRJoJFwDAzG0LwhX8NMDmpzmzgK8AbwJXAX92jmSR68riBSgAiMefunX7iuUP5Co2sa8jd9wO3AM8Bq4An3X2Fmd1jZpPCao8CpWa2BrgNaHKLqYhIeygpKaG6uvqQvijzRf16BCUlJa36nOXbD6WsrMwXL16c6zBEJM/EfYUyMyt397JUn9GTxSISC0VFRa1atStO8uKuIRERiY4SgYhIzCkRiIjEXN4NFpvZVqDyED/em6SnlmNA1xwPuuZ4aMs1D3L3lE/k5l0iaAszW9zcqHlnpWuOB11zPER1zeoaEhGJOSUCEZGYi1simJbrAHJA1xwPuuZ4iOSaYzVGICIiTcWtRSAiIkmUCEREYq5TJgIzu8jMVpvZGjNrMqOpmR1mZk+E+xea2eDsR9m+Mrjm28xspZktM7MXzWxQLuJsTy1dc0K9L5qZm1ne32qYyTWb2VXh3/UKM5uR7RjbWwb/tgea2UtmtiT8931xLuJsL2Y23cy2mNnyZvabmT0Y/jyWmdlpbT6pu3eqF1AIvAcMBYqBt4ERSXW+Bfw6fH8N8ESu487CNZ8LHB6+/2Ycrjms1x14BVgAlOU67iz8PQ8DlgBHhuWjcx13Fq55GvDN8P0IYG2u427jNZ8NnAYsb2b/xcBcwIDxwMK2nrMztgjGAmvcvcLd9wEzgcuS6lwG/DZ8/yfgfMvv1SpavGZ3f8ndd4XFBQQrxuWzTP6eAX4I/BjoDHMPZ3LNXwcecvcaAHffkuUY21sm1+xAj/B9T2BjFuNrd+7+CvBRmiqXAb/zwAKgl5n1bcs5O2Mi6A+sTyhXhdtS1vFgAZ3tQGlWootGJtec6AaC3yjyWYvXHDaZj3P3Z7IZWIQy+XseDgw3s9fMbIGZXZS16KKRyTV/H7jOzKqAOcCt2QktZ1r7/71FWo8gZszsOqAMOCfXsUTJzAqAnwPX50JSbMcAAAWsSURBVDiUbOtC0D00gaDV94qZneLu23IaVbSuBR5z95+Z2enA781spLvX5TqwfNEZWwQbgOMSygPCbSnrmFkXguZkdVaii0Ym14yZfQ64C5jk7nuzFFtUWrrm7sBIYJ6ZrSXoS52d5wPGmfw9VwGz3b3W3d8H3iVIDPkqk2u+AXgSwN3fAEoIJmfrrDL6/94anTERLAKGmdkQMysmGAyenVRnNvCV8P2VwF89HIXJUy1es5mdCjxMkATyvd8YWrhmd9/u7r3dfbC7DyYYF5nk7vm8zmkm/7ZnEbQGMLPeBF1FFdkMsp1lcs3rgPMBzOwkgkSwNatRZtds4Mvh3UPjge3uvqktB+x0XUPuvt/MbgGeI7jjYLq7rzCze4DF7j4beJSg+biGYFDmmtxF3HYZXvMDwBHAH8Nx8XXuPilnQbdRhtfcqWR4zc8BF5rZSuAAcIe7521rN8Nrvh34jZn9H4KB4+vz+Rc7M/sDQTLvHY57fA8oAnD3XxOMg1wMrAF2Af/U5nPm8c9LRETaQWfsGhIRkVZQIhARiTklAhGRmFMiEBGJOSUCEZGYUyKQDsnMDpjZ0oTX4DR1P2mH8z1mZu+H53orfEK1tcd4xMxGhO+/m7Tv9bbGGB6n/uey3MyeNrNeLdQfne+zcUr0dPuodEhm9om7H9HeddMc4zHgL+7+JzO7EPipu49qw/HaHFNLxzWz3wLvuvt9aepfTzDr6i3tHYt0HmoRSF4wsyPCdRTeMrN3zKzJTKNm1tfMXkn4jfmz4fYLzeyN8LN/NLOWvqBfAU4IP3tbeKzlZvbtcFs3M3vGzN4Ot18dbp9nZmVmNhXoGsbxeLjvk/DPmWZ2SULMj5nZlWZWaGYPmNmicI75b2TwY3mDcLIxMxsbXuMSM3vdzE4Mn8S9B7g6jOXqMPbpZvZmWDfVjK0SN7mee1svvVK9CJ6KXRq+niJ4Cr5HuK83wVOV9S3aT8I/bwfuCt8XEsw31Jvgi71buP3fgLtTnO8x4Mrw/T8AC4ExwDtAN4KnslcApwJfBH6T8Nme4Z/zCNc8qI8poU59jFcAvw3fFxPMItkVuBGYEm4/DFgMDEkR5ycJ1/dH4KKw3APoEr7/HPDn8P31wC8TPn8/cF34vhfBXETdcv33rVduX51uignpNHa7++j6gpkVAfeb2dlAHcFvwscAmxM+swiYHtad5e5LzewcgsVKXgun1igm+E06lQfMbArBPDU3EMxf85S77wxj+F/gs8CzwM/M7McE3UmvtuK65gL/aWaHARcBr7j77rA7apSZXRnW60kwWdz7SZ/vamZLw+tfBTyfUP+3ZjaMYJqFombOfyEwycy+E5ZLgIHhsSSmlAgkX3wJ6AOMcfdaC2YULUms4O6vhIniEuAxM/s5UAM87+7XZnCOO9z9T/UFMzs/VSV3f9eCtQ4uBu41sxfd/Z5MLsLd95jZPODzwNUEC61AsNrUre7+XAuH2O3uo83scIL5d24GHiRYgOcld78iHFif18znDfiiu6/OJF6JB40RSL7oCWwJk8C5QJM1ly1Yh/kDd/8N8AjBcn8LgDPNrL7Pv5uZDc/wnK8Cl5vZ4WbWjaBb51Uz6wfscvf/IZjML9WasbVhyySVJwgmCqtvXUDwpf7N+s+Y2fDwnCl5sNrcPwO328Gp1OunIr4+oeoOgi6yes8Bt1rYPLJgVlqJOSUCyRePA2Vm9g7wZeBvKepMAN42syUEv23/p7tvJfhi/IOZLSPoFvpUJid097cIxg7eJBgzeMTdlwCnAG+GXTTfA+5N8fFpwLL6weIk/49gYaAXPFh+EYLEtRJ4y4JFyx+mhRZ7GMsygoVZfgL8KLz2xM+9BIyoHywmaDkUhbGtCMsSc7p9VEQk5tQiEBGJOSUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJuf8PlJvv/2pHasYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}